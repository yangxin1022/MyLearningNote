{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Profiler is a tool that allows the collection of the performace metrics during the training and inference. Profiler's context manager API can be used to better understand what model operators are the most expensive, examine their input shapes and stack traces, study device kernel activity and visualize the execution trace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/master/profiler.html  \n",
    "https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html  \n",
    "https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "\n",
    "1. Import all necessary libraries\n",
    "2. Instantiate a simple Resnet model\n",
    "3. Using profiler to analyze execution time\n",
    "4. Using profiler to analyze memory consumption\n",
    "5. Using tracing functionality\n",
    "6. Examining stack traces\n",
    "7. Visualizating data as flmegraph\n",
    "8. Using profiler to analyze long-running job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import all necessary libraries\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Instantiate a simple Resnet model\n",
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 244, 244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Using profiler to analyze execution time\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch profiler is enableed through the context manager and accepts a number of parameters, some of the most useful are:  \n",
    "- `activities` - a list of activities to profile:\n",
    "  - `ProfilerActivity.CPU` - PyTorch operators, TorchScript functions and user-defined code labels\n",
    "  - `ProfilerActivity.CUDA` - on-device CUDA kernels\n",
    "- `record-shapes` - whether to record shapes of the operator inputs\n",
    "- `profile_memory` - whether to report amount of memory consumed by model's Tensors;\n",
    "- `use_cuda` - whether to measure excution time of CUDA kernels.\n",
    "\n",
    "Note that we can use `record_function` context manager to label arbitrary code ranges with user provided names(`model_inference` is used as a label in the example above).  \n",
    "Profiler allows one to check which operators were called during the execution of a code range wrapped with a profiler context manager. If multiple profiler ranges are active at the same time (e.g. in parallel PyTorch threads), each profiling context manager tracks only the operators of its corresponding range. Profiler also automatically profiles the async tasks launched with `torch.jit._fork` and (in case of a backward pass) the backward pass operators launched with `backward()` call. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference         6.65%      18.580ms        99.33%     277.680ms     277.680ms             1  \n",
      "                     aten::conv2d         0.27%     757.000us        64.32%     179.799ms       8.990ms            20  \n",
      "                aten::convolution         0.69%       1.938ms        64.05%     179.042ms       8.952ms            20  \n",
      "               aten::_convolution         0.26%     730.000us        63.35%     177.104ms       8.855ms            20  \n",
      "         aten::mkldnn_convolution        62.92%     175.907ms        63.09%     176.374ms       8.819ms            20  \n",
      "                 aten::batch_norm         0.36%       1.014ms        18.35%      51.290ms       2.564ms            20  \n",
      "     aten::_batch_norm_impl_index         0.49%       1.362ms        17.98%      50.276ms       2.514ms            20  \n",
      "          aten::native_batch_norm        13.95%      39.003ms        17.48%      48.873ms       2.444ms            20  \n",
      "                 aten::max_pool2d         0.06%     178.000us         5.20%      14.538ms      14.538ms             1  \n",
      "    aten::max_pool2d_with_indices         5.14%      14.360ms         5.14%      14.360ms      14.360ms             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 279.554ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that, as expected, most of the time is spent in convolution (and specifically in `mkldnn_convolution` for PyTorch compiled with MKL-DNN support). Note the difference between self cpu time and cpu time - operators can call other operators, self cpu time exludes time spent in children operator calls, while total cpu time includes it. You can choose to sort by the self cpu time by passing `sort_by=\"self_cpu_time_total\"` into the table call.\n",
    "\n",
    "To get a finer granularity of results and include operator input shapes, pass `group_by_input_shape=True` (note: this requires running the profiler with `record_shapes=True`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                                                      Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                  model_inference         6.65%      18.580ms        99.33%     277.680ms     277.680ms             1                                                                                []  \n",
      "                     aten::conv2d         0.01%      35.000us        13.49%      37.711ms       9.428ms             4                             [[5, 64, 61, 61], [64, 64, 3, 3], [], [], [], [], []]  \n",
      "                aten::convolution         0.05%     126.000us        13.48%      37.676ms       9.419ms             4                     [[5, 64, 61, 61], [64, 64, 3, 3], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.02%      62.000us        13.43%      37.550ms       9.387ms             4     [[5, 64, 61, 61], [64, 64, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
      "         aten::mkldnn_convolution        13.38%      37.410ms        13.41%      37.488ms       9.372ms             4                             [[5, 64, 61, 61], [64, 64, 3, 3], [], [], [], [], []]  \n",
      "                     aten::conv2d         0.01%      25.000us        12.90%      36.067ms      12.022ms             3                            [[5, 512, 8, 8], [512, 512, 3, 3], [], [], [], [], []]  \n",
      "                aten::convolution         0.03%      87.000us        12.89%      36.042ms      12.014ms             3                    [[5, 512, 8, 8], [512, 512, 3, 3], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.01%      40.000us        12.86%      35.955ms      11.985ms             3    [[5, 512, 8, 8], [512, 512, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
      "         aten::mkldnn_convolution        12.83%      35.880ms        12.85%      35.915ms      11.972ms             3                            [[5, 512, 8, 8], [512, 512, 3, 3], [], [], [], [], []]  \n",
      "                     aten::conv2d         0.21%     586.000us        11.31%      31.623ms      31.623ms             1                             [[5, 3, 244, 244], [64, 3, 7, 7], [], [], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "Self CPU time total: 279.554ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference         0.14%       3.497ms        99.90%        2.566s        2.566s     756.000us         0.03%        2.568s        2.568s             1  \n",
      "                     aten::conv2d         0.04%       1.127ms        51.68%        1.327s      66.375ms     855.000us         0.03%        1.355s      67.734ms            20  \n",
      "                aten::convolution         0.03%     726.000us        51.64%        1.326s      66.319ms     341.000us         0.01%        1.354s      67.691ms            20  \n",
      "               aten::_convolution         0.06%       1.547ms        51.61%        1.326s      66.282ms       1.275ms         0.05%        1.353s      67.674ms            20  \n",
      "          aten::cudnn_convolution        51.52%        1.323s        51.55%        1.324s      66.205ms        1.352s        52.59%        1.352s      67.611ms            20  \n",
      "                     aten::linear         0.00%      41.000us        41.18%        1.058s        1.058s      23.000us         0.00%        1.021s        1.021s             1  \n",
      "                      aten::addmm        41.18%        1.058s        41.18%        1.058s        1.058s        1.021s        39.74%        1.021s        1.021s             1  \n",
      "                 aten::batch_norm         0.02%     535.000us         6.61%     169.692ms       8.485ms     284.000us         0.01%     175.440ms       8.772ms            20  \n",
      "     aten::_batch_norm_impl_index         0.04%       1.090ms         6.59%     169.157ms       8.458ms     886.000us         0.03%     175.156ms       8.758ms            20  \n",
      "           aten::cudnn_batch_norm         6.42%     165.003ms         6.54%     168.067ms       8.403ms     170.281ms         6.62%     174.270ms       8.713ms            20  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.568s\n",
      "Self CUDA time total: 2.570s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Profiler can also be used to analyze performance of models executed on GPUs:\n",
    "model = models.resnet18().cuda()\n",
    "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
    "\n",
    "with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                      aten::empty         0.63%       1.750ms         0.63%       1.750ms      10.938us      94.84 Mb      94.84 Mb           160  \n",
      "    aten::max_pool2d_with_indices         4.10%      11.424ms         4.10%      11.424ms      11.424ms      11.48 Mb      11.48 Mb             1  \n",
      "                       aten::mean         0.42%       1.157ms         7.06%      19.646ms     935.524us      28.75 Kb      28.75 Kb            21  \n",
      "                      aten::addmm         1.17%       3.270ms         1.19%       3.319ms       3.319ms      19.53 Kb      19.53 Kb             1  \n",
      "              aten::empty_strided         0.06%     172.000us         0.06%     172.000us       8.190us          84 b          84 b            21  \n",
      "                     aten::conv2d         0.08%     228.000us        59.18%     164.755ms       8.238ms      47.37 Mb           0 b            20  \n",
      "                aten::convolution         0.19%     521.000us        59.10%     164.527ms       8.226ms      47.37 Mb           0 b            20  \n",
      "               aten::_convolution         0.10%     290.000us        58.92%     164.006ms       8.200ms      47.37 Mb           0 b            20  \n",
      "         aten::mkldnn_convolution        58.67%     163.335ms        58.81%     163.716ms       8.186ms      47.37 Mb           0 b            20  \n",
      "                aten::as_strided_         0.01%      39.000us         0.01%      39.000us       1.950us           0 b           0 b            20  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 278.376ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Using profiler to analyze memory consumption\n",
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 224, 224)\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU],\n",
    "        profile_memory=True, record_shapes=True) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                      aten::empty         0.63%       1.750ms         0.63%       1.750ms      10.938us      94.84 Mb      94.84 Mb           160  \n",
      "                 aten::batch_norm         0.05%     130.000us        26.88%      74.840ms       3.742ms      47.41 Mb           0 b            20  \n",
      "     aten::_batch_norm_impl_index         0.10%     278.000us        26.84%      74.710ms       3.736ms      47.41 Mb           0 b            20  \n",
      "          aten::native_batch_norm        20.00%      55.681ms        26.72%      74.389ms       3.719ms      47.41 Mb     -75.00 Kb            20  \n",
      "                     aten::conv2d         0.08%     228.000us        59.18%     164.755ms       8.238ms      47.37 Mb           0 b            20  \n",
      "                aten::convolution         0.19%     521.000us        59.10%     164.527ms       8.226ms      47.37 Mb           0 b            20  \n",
      "               aten::_convolution         0.10%     290.000us        58.92%     164.006ms       8.200ms      47.37 Mb           0 b            20  \n",
      "         aten::mkldnn_convolution        58.67%     163.335ms        58.81%     163.716ms       8.186ms      47.37 Mb           0 b            20  \n",
      "                 aten::empty_like         0.05%     142.000us         0.18%     494.000us      24.700us      47.37 Mb           0 b            20  \n",
      "                 aten::max_pool2d         0.00%      12.000us         4.11%      11.436ms      11.436ms      11.48 Mb           0 b             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 278.376ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch profiler can also show the amount of memory (used by the model’s tensors) that was allocated (or released) during the execution of the model’s operators. In the output below, ‘self’ memory corresponds to the memory allocated (released) by the operator, excluding the children calls to the other operators. To enable memory profiling functionality pass `profile_memory=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Using tracing functionality\n",
    "# Profiling results can be outputted as a .json trace file:\n",
    "model = models.resnet18().cuda()\n",
    "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Source Location                                                              \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  \n",
      "          aten::cudnn_convolution        23.98%       2.592ms        26.48%       2.863ms     143.150us      39.396ms        57.14%      39.532ms       1.977ms            20  lib\\runpy.py(75): _run_code                                                  \n",
      "                                                                                                                                                                               lib\\site-packages\\ipykernel_launcher.py(12): <module>                        \n",
      "                                                                                                                                                                               lib\\site-packages\\traitlets\\config\\application.py(844): launch_instance      \n",
      "                                                                                                                                                                               lib\\site-packages\\ipykernel\\kernelapp.py(702): start                         \n",
      "                                                                                                                                                                               lib\\site-packages\\tornado\\platform\\asyncio.py(195): start                    \n",
      "                                                                                                                                                                                                                                                            \n",
      "           aten::cudnn_batch_norm        17.30%       1.870ms        30.61%       3.309ms     165.450us      15.494ms        22.47%      17.828ms     891.400us            20  lib\\runpy.py(75): _run_code                                                  \n",
      "                                                                                                                                                                               lib\\site-packages\\ipykernel_launcher.py(12): <module>                        \n",
      "                                                                                                                                                                               lib\\site-packages\\traitlets\\config\\application.py(844): launch_instance      \n",
      "                                                                                                                                                                               lib\\site-packages\\ipykernel\\kernelapp.py(702): start                         \n",
      "                                                                                                                                                                               lib\\site-packages\\tornado\\platform\\asyncio.py(195): start                    \n",
      "                                                                                                                                                                                                                                                            \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  \n",
      "Self CPU time total: 10.811ms\n",
      "Self CUDA time total: 68.950ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Examing stack traces\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    with_stack=True,\n",
    ") as prof:\n",
    "    model(inputs)\n",
    "\n",
    "# Print aggregated stats\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/profiler_stacks.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Python_learning\\MyLearningNote\\pytorch-tutorial\\ProfilingPytorchModule.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Python_learning/MyLearningNote/pytorch-tutorial/ProfilingPytorchModule.ipynb#ch0000033?line=0'>1</a>\u001b[0m \u001b[39m# 7. Visualizing data as a flamegraph\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Python_learning/MyLearningNote/pytorch-tutorial/ProfilingPytorchModule.ipynb#ch0000033?line=1'>2</a>\u001b[0m prof\u001b[39m.\u001b[39;49mexport_stacks(\u001b[39m\"\u001b[39;49m\u001b[39m/tmp/profiler_stacks.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mself_cuda_time_total\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\d2l\\lib\\site-packages\\torch\\profiler\\profiler.py:138\u001b[0m, in \u001b[0;36m_KinetoProfile.export_stacks\u001b[1;34m(self, path, metric)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/profiler/profiler.py?line=123'>124</a>\u001b[0m \u001b[39m\"\"\"Save stack traces in a file in a format suitable for visualization.\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/profiler/profiler.py?line=124'>125</a>\u001b[0m \n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/profiler/profiler.py?line=125'>126</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/profiler/profiler.py?line=134'>135</a>\u001b[0m \u001b[39m    - ./flamegraph.pl --title \"CPU time\" --countname \"us.\" profiler.stacks > perf_viz.svg\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/profiler/profiler.py?line=135'>136</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/profiler/profiler.py?line=136'>137</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/profiler/profiler.py?line=137'>138</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mexport_stacks(path, metric)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\profiler.py:247\u001b[0m, in \u001b[0;36mprofile.export_stacks\u001b[1;34m(self, path, metric)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/autograd/profiler.py?line=244'>245</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_events \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mExpected profiling results\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/autograd/profiler.py?line=245'>246</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_stack, \u001b[39m\"\u001b[39m\u001b[39mexport_stacks() requires with_stack=True\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/autograd/profiler.py?line=246'>247</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_events\u001b[39m.\u001b[39;49mexport_stacks(path, metric)\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\profiler_util.py:239\u001b[0m, in \u001b[0;36mEventList.export_stacks\u001b[1;34m(self, path, metric)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/autograd/profiler_util.py?line=236'>237</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmetric should be one of: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupported_export_stacks_metrics()))\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/autograd/profiler_util.py?line=237'>238</a>\u001b[0m translate_table \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mmaketrans(\u001b[39m\"\u001b[39m\u001b[39m ;\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m____\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/autograd/profiler_util.py?line=238'>239</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/autograd/profiler_util.py?line=239'>240</a>\u001b[0m     \u001b[39mfor\u001b[39;00m evt \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m    <a href='file:///e%3A/Anaconda/envs/d2l/lib/site-packages/torch/autograd/profiler_util.py?line=240'>241</a>\u001b[0m         \u001b[39mif\u001b[39;00m evt\u001b[39m.\u001b[39mstack \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(evt\u001b[39m.\u001b[39mstack) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/profiler_stacks.txt'"
     ]
    }
   ],
   "source": [
    "# 7. Visualizing data as a flamegraph\n",
    "prof.export_stacks(\"/tmp/profiler_stacks.txt\", \"self_cuda_time_total\")\n",
    "# We recommend using e.g. Flamegraph tool to generate an interactive SVG:\n",
    "# git clone https://github.com/brendangregg/FlameGraph\n",
    "# cd FlameGraph\n",
    "# ./flamegraph.pl --title \"CUDA time\" --countname \"us.\" /tmp/profiler_stacks.txt > perf_viz.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Using profiler to analyze long-running jobs\n",
    "from torch.profiler import schedule\n",
    "\n",
    "my_schedule = schedule(\n",
    "    skip_first=10,\n",
    "    wait=5,\n",
    "    warmup=1,\n",
    "    active=3,\n",
    "    repeat=2)\n",
    "\n",
    "def trace_handler(p):\n",
    "    output = p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10)\n",
    "    print(output)\n",
    "    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=2),\n",
    "    on_trace_ready=trace_handler\n",
    ") as p:\n",
    "    for idx in range(8):\n",
    "        model(inputs)\n",
    "        p.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch profiler offers an additional API to handle long-running jobs (such as training loops). Tracing all of the execution can be slow and result in very large trace files. To avoid this, use optional arguments:  \n",
    "- `schedule` - specifies a function that takes an integer argument (step number) as an input and returns an action for the profiler, the best way to use this parameter is to use\n",
    "- `on_trace_ready` - specifies a function that takes a reference to the profiler as an input and is called by the profiler each time the new trace is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiler assumes that the long-running job is composed of steps, numbered starting from zero. The example above defines the following sequence of actions for the profiler:  \n",
    "1. Parameter `skip_first` tells profiler that it should ignore the first 10 steps (default value of `skip_first` is zero);\n",
    "2. After the first `skip_first` steps, profiler starts executing profiler cycles;\n",
    "3. Each cycle consists of three phases:\n",
    "   1. idling (wait=5 steps), during this phase profiler is not active;\n",
    "   2. warming up (warmup=1 steps), during this phase profiler starts tracing, but the results are discarded; this phase is used to discard the samples obtained by the profiler at the beginning of the trace since they are usually skewed by an extra overhead;\n",
    "   3. active tracing (active=3 steps), during this phase profiler traces and records data;\n",
    "4. An optional repeat parameter specifies an upper bound on the number of cycles. By default (zero value), profiler will execute cycles as long as the job runs.\n",
    "   \n",
    "Thus, in the example above, profiler will skip the first 15 steps, spend the next step on the warm up, actively record the next 3 steps, skip another 5 steps, spend the next step on the warm up, actively record another 3 steps. Since the repeat=2 parameter value is specified, the profiler will stop the recording after the first two cycles.\n",
    "\n",
    "At the end of each cycle profiler calls the specified on_trace_ready function and passes itself as an argument. This function is used to process the new trace - either by obtaining the table output or by saving the output on disk as a trace file.\n",
    "\n",
    "To send the signal to the profiler that the next step has started, call prof.step() function. The current profiler step is stored in prof.step_num."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e3fb3b94af17b2336c49a5de82acaa44c3b8f6e324b719a96125783e256d8bd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
