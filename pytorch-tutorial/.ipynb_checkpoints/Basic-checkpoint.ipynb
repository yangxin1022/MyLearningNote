{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From PyTorch offical tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are similar to numpy's ndarrays, except that tensors can run on GPUs or other hardware accelerateors. In fact, tensors and numpy arrays can often share the same underlying memory, eliminating the need to copy data. Tensors are also optimized fro automatic differentiation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.2896, 0.1194],\n",
      "        [0.9953, 0.3989]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.5887, 0.6228, 0.7574],\n",
      "        [0.6672, 0.0887, 0.6618]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Initializing a tensor\n",
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n",
    "\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)\n",
    "\n",
    "# The new tensor retains the properties (shape, datatype) of the argument \n",
    "# tensor, unless explicitly overridden\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n",
    "shape = (2, 3, )\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attributes of a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tensor: torch.Size([3, 4])\n",
      "datatype of tensor: torch.float32\n",
      "device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"shape of tensor: {tensor.shape}\")\n",
    "print(f\"datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Operations on Tensors\n",
    "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using .to method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column:  tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# standard numpy-like indexing and slicing\n",
    "tensor = torch.ones(4, 4)\n",
    "print('First row: ', tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column: ', tensor[..., -1])\n",
    "tensor[:, 1] = 0\n",
    "print(tensor)\n",
    "\n",
    "# you can use torch.cat to concatenate a sequence of tensors along a given dimension\n",
    "t1 = torch.cat([tensor, tensor, tensor, tensor])\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Arithmetic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below compute the matrix multiplication between two tensors. y1, y2, y3 will have the same value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = tensor@tensor.T\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = tensor.matmul(tensor.T)\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single-element tensors If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using item():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-place operations Operations that store the result into the operand are called in-place. They are denoted by a _ suffix. For example: x.copy_(y), x.t_(), will change x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, '\\n')\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bridge with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t.add(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Fashion-MNIST dataset as an example\n",
    "# root is the path where the train/test data is stored\n",
    "# train specifies training or test dataset\n",
    "# download=True downloads the data from the internet if it's not available at root\n",
    "# transform and target_transform specify the feature and label transformations\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iterating and visualizing the dataset\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\"\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols*rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Createing a Custom Dataset for your files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom Dataset class must implement three functions: `__init__`, `__len__`, and `__getitem__`. Take a look at this implementation; the FashionMNIST images are stored in a directory img_dir, and their labels are stored separately in a CSV file annotations_file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_trasform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_trasform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__init__` function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section).\n",
    "\n",
    "The `__len__` function returns the number of samples in our dataset.\n",
    "\n",
    "The `__getitem__` function loads and returns a sample from the dataset at the given index idx. Based on the index, it identifies the image’s location on disk, converts that to a tensor using read_image, retrieves the corresponding label from the csv data in self.img_labels, calls the transform functions on them (if applicable), and returns the tensor image and corresponding label in a tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前后都有双下划线的method被称为python的魔法方法，独立于其他方法之外，可以快速实现一些功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing your data for training with DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\n",
    "\n",
    "DataLoader is an iterable that abstracts this complexity for us in an easy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate through the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(train_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data does not always come in its final processed form that is required for training machine learning algorithms. We use transforms to perform some manipulation of the data and make it suitable for training.\n",
    "\n",
    "All TorchVision datasets have two parameters -`transform` to modify the features and `target_transform` to modify the labels - that accept callables containing the transformation logic. The torchvision.transforms module offers several commonly-used transforms out of the box.\n",
    "\n",
    "The FashionMNIST features are in PIL Image format, and the labels are integers. For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors. To make these transformations, we use `ToTensor` and `Lambda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y),\n",
    "                                                                                  value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ToTensor` converts a PIL image or NumPy ndarray into a FloatTensor. and scales the image’s pixel intensity values in the range [0., 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot encoded tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls `scatter_` which assigns a `value=1` on the index as given by the label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Neural networks comprise of layers/modules that perform operations on data. The `torch.nn` namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the `nn.Module`. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every nn.Module subclass implements the operations on input data in the `forward` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About `forward` method:   \n",
    "The `forward` function computes output Tensors from input Tensors. The `backward` function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# We create an instance of NeuralNetwork, and move it to the device, and print its structure. \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classL tensor([1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# To use the model, we pass it the input data. This executes the model's forward, along with some background operations. Do not call model.forward()\n",
    "# directly\n",
    "# Calling the model on the input returns a 10-dimensional tensor with raw predicted values for each class. We get the prediction probabilities by\n",
    "# passing it through an instance of the nn.Softmax module. \n",
    "X = torch.rand(1, 28, 28, device = device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network.\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "# nn.Flatten\n",
    "# We initialize the nn.Flatten layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (the minibatch dimension (at dim=0) is maintained). \n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# nn.Linear\n",
    "# The linear layer is a module that applies a linear transformation on the input using its stored weights and biases\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.0033,  0.1760, -0.2938, -0.0321,  0.0853,  0.7193,  0.0235,  0.3339,\n",
      "          0.0902, -0.0398,  0.0854, -0.0253, -0.1703,  0.0825,  0.2438, -0.2604,\n",
      "          0.1628, -0.0276, -0.0752,  0.6937],\n",
      "        [-0.0055, -0.0312, -0.0032, -0.0677, -0.1443,  0.6762,  0.0926, -0.2029,\n",
      "          0.0611,  0.1954,  0.1149, -0.2006, -0.0340,  0.2781, -0.0517, -0.6163,\n",
      "          0.3356,  0.1091,  0.0222,  0.4169],\n",
      "        [ 0.3286,  0.0870,  0.1596,  0.2810, -0.1203,  0.5633,  0.0436, -0.0596,\n",
      "          0.0392,  0.1478,  0.2603, -0.3855,  0.0158, -0.1003,  0.1511, -0.3299,\n",
      "         -0.0080,  0.1806, -0.1143,  1.0790]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0033, 0.1760, 0.0000, 0.0000, 0.0853, 0.7193, 0.0235, 0.3339, 0.0902,\n",
      "         0.0000, 0.0854, 0.0000, 0.0000, 0.0825, 0.2438, 0.0000, 0.1628, 0.0000,\n",
      "         0.0000, 0.6937],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6762, 0.0926, 0.0000, 0.0611,\n",
      "         0.1954, 0.1149, 0.0000, 0.0000, 0.2781, 0.0000, 0.0000, 0.3356, 0.1091,\n",
      "         0.0222, 0.4169],\n",
      "        [0.3286, 0.0870, 0.1596, 0.2810, 0.0000, 0.5633, 0.0436, 0.0000, 0.0392,\n",
      "         0.1478, 0.2603, 0.0000, 0.0158, 0.0000, 0.1511, 0.0000, 0.0000, 0.1806,\n",
      "         0.0000, 1.0790]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# nn.ReLU\n",
    "# Non-linear activations are what create the complex mappings between the model's inputs and putputs. They are applied after linear transformations to introduce nonlinearity,\n",
    "# helping neural networks learn a wide variety of phenomena. \n",
    "# In this model, we use nn.ReLU between our linear layers, but there's other activations to introduce non-linearity in your model. \n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential\n",
    "# nn.Sequential is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together\n",
    "# a quick network like seq_modules. \n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Softmax\n",
    "# The last linear layer of the neural network returns logits -raw values in [-infty, infty] - which are passed to the nn.Softmax module. \n",
    "# The logits are scaled to values [0, 1] representing the model's predicted probabilities for each class. dim parameter indicates the dimension along\n",
    "# which the values must sum to 1. \n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1055, 0.0595, 0.0978, 0.1445, 0.0946, 0.1097, 0.0779, 0.0899, 0.1277,\n",
       "         0.0928],\n",
       "        [0.1035, 0.0661, 0.0927, 0.1449, 0.0941, 0.1095, 0.0722, 0.1026, 0.1172,\n",
       "         0.0973],\n",
       "        [0.1088, 0.0673, 0.0925, 0.1322, 0.0972, 0.1201, 0.0761, 0.0882, 0.1164,\n",
       "         0.1012]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight |  Size: torch.Size([512, 784]) | Values : tensor([[-0.0011,  0.0137,  0.0012,  ...,  0.0195, -0.0345,  0.0165],\n",
      "        [-0.0296,  0.0141, -0.0130,  ...,  0.0147, -0.0001, -0.0033]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias |  Size: torch.Size([512]) | Values : tensor([0.0012, 0.0125], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight |  Size: torch.Size([512, 512]) | Values : tensor([[-0.0324, -0.0178, -0.0337,  ..., -0.0244, -0.0075,  0.0230],\n",
      "        [ 0.0279,  0.0360,  0.0240,  ..., -0.0266, -0.0101, -0.0400]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias |  Size: torch.Size([512]) | Values : tensor([-0.0376,  0.0252], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight |  Size: torch.Size([10, 512]) | Values : tensor([[-0.0078,  0.0122, -0.0025,  ...,  0.0012, -0.0300,  0.0091],\n",
      "        [ 0.0438,  0.0083,  0.0323,  ...,  0.0028,  0.0220,  0.0185]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias |  Size: torch.Size([10]) | Values : tensor([0.0427, 0.0286], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing nn.Module automatically\n",
    "# tracks all fields defined inside your model object, and makes all parameters accessible using your model's parameters() or named_parameters() methods. \n",
    "\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} |  Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.   \n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called `torch.autograd`. It supports automatic computation of gradient for any computational graph.  \n",
    "Consider the simplest one-layer neoral network, with input x, parameters w and b, and some loss function. It can be defined in PyTorch in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "# torch.nn.functional.binary_cross_entropy_with_logits\n",
    "# Function that measures Binary Cross Entropy between target and input logits. \n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4650, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_1628/1947561727.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\YANG~1.XIN\\AppData\\Local\\Temp/ipykernel_1628/1947561727.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    (./image/uTools_1652255130847.png)\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "![img]\n",
    "(./image/uTools_1652255130847.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this network, w and b are parameters, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables. In order to do that, we set the `requires_grad` property of those tensors.   \n",
    "You can set the value of requires_grad when creating a tensor, or later by using `x.requires_grad_(True)` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that we apply to tensors to construct computational graph is infact an object of class `Function`. This pbject knows how to compute the function in the forward direction, and also how to compute its derivative during the backward propagation step. A reference to the backward propagation function is stored in `grad_fun` property of a tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000002E63072BA60>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000002E63072A3B0>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respoect to parameters, namely, we need $\\frac{\\partial{loss}}{\\partial{\\omega}}$ and $\\frac{\\partial{loss}}{\\partial{b}}$ under some fixed values of x and y. To compute those derivatives, we call `loss.backward()`, and then retrieve the values from `w.grad` and `b.grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0565, 0.1651, 0.1362],\n",
      "        [0.0565, 0.1651, 0.1362],\n",
      "        [0.0565, 0.1651, 0.1362],\n",
      "        [0.0565, 0.1651, 0.1362],\n",
      "        [0.0565, 0.1651, 0.1362]])\n",
      "tensor([0.0565, 0.1651, 0.1362])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes from pytorch-tutoiral Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = w * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: Parameter containing:\n",
      "tensor([[ 0.2988, -0.4398, -0.4501],\n",
      "        [-0.5166, -0.2803,  0.1150]], requires_grad=True)\n",
      "b: Parameter containing:\n",
      "tensor([0.3140, 0.1766], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(3, 2)\n",
    "print('w:', linear.weight)\n",
    "print('b:', linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.1694972515106201\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw:  tensor([[ 0.5894,  0.4381, -0.5044],\n",
      "        [-0.4589, -0.6812,  0.6138]])\n",
      "dL/db:  tensor([ 0.8289, -0.2027])\n"
     ]
    }
   ],
   "source": [
    "print('dL/dw: ', linear.weight.grad)\n",
    "print('dL/db: ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-step gradient descent.\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 step optimizationL  1.1440227031707764\n"
     ]
    }
   ],
   "source": [
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimizationL ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2975, 0.1807])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also perform gradient descent at the low level.\n",
    "linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "linear.bias.data.sub_(0.01 * linear.bias.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 step optimization:  1.1190683841705322\n"
     ]
    }
   ],
   "source": [
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.sub`是做减法的一个函数，`torch.sub_`是sub的in-place版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading data from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2], [3, 4]])\n",
    "y = torch.from_numpy(x) # Convert the numpy array to a torch tensor\n",
    "z = y.numpy() # Convert the torch tensor to a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170499072it [00:07, 24137445.98it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/cifar-10-python.tar.gz to ../../data/\n"
     ]
    }
   ],
   "source": [
    "# Download and construct CIFAR - 10 dataset. \n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True,\n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Fetch one data pair (read data from disk)\n",
    "image, label = train_dataset[0]\n",
    "print(image.size())\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader (this provides queues and threads in a very simple way)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch images and labels\n",
    "images, labels = data_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23dec91ba60>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets plot one image to check (well, it seems like a frog)\n",
    "import matplotlib.pyplot as plt\n",
    "img = image.numpy()\n",
    "plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "# numpy.transpose reverse or permute the axes of an array;\n",
    "# returns the modified array.\n",
    "# 简而言之就是转置，上面的作用就是把原来的轴的顺序掉换成2, 3, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actural usage of the data loader is as below.\n",
    "for images, labels in train_loader:\n",
    "    # Training code should be written here.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Input pipeline for custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should builf your custom dataset as below\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        # 1. Initialize file paths or a list of file names\n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprcess the data(e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label)\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # you should change 0 to the total size of your dataset. \n",
    "        return 0\n",
    "    \n",
    "# you can then use the prebuilt data loader\n",
    "custom_dataset = CustomDataset()\n",
    "tran_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                          batch_size = 64,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the pretrained ResNet - 18\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# If you want to finetune only the top layer of the model, set as below\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the top layer for finetuning. \n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100) # 100 is an example\n",
    "\n",
    "# Forward pass\n",
    "images = torch.randn(64, 3, 244, 244)\n",
    "outputs = resnet(images)\n",
    "print(outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load the entire model\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "model = torch.load('model.ckpt')\n",
    "\n",
    "# Save and load only the model parameters (recommended)\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e3fb3b94af17b2336c49a5de82acaa44c3b8f6e324b719a96125783e256d8bd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
